# Data Engineering<br>
**Data Engineering Capstone on data platform for retailer data analytics for an e-commerce organization** <br>

### Project Documentation
1. Designed a data platform that uses MySQL as an OLTP database and MongoDB as a NoSQL database.
    - design the OLTP database for an E-Commerce website, 
    - populate the OLTP Database with the data provided and 
    - automate the load/export of the daily incremental data into the data warehouse

2. Designed and implemented a data warehouse and generated reports from the data.
    - set up a NoSQL database to store the catalog data for an E-Commerce website,
    - load the E-Commerce catalog data into the NoSQL database, and 
    - query the E-Commerce catalog data in the NoSQL database.

3. Designed a reporting dashboard that reflects the key metrics of the business.
    - design the schema for a data warehouse based on the schema of the OLTP and NoSQL databases.
    - then create the schema and load the data into fact and dimension tables, automate the daily incremental data insertion into the data warehouse, and
    - create cubes and rollups to make reporting easier.
    - create a Cognos data source that points to a data warehouse table
    - create a bar chart of Quarterly sales of cell phones, 
    - create a pie chart of sales of electronic goods by category, and
    - create a line chart of total sales per month for the year 2020.

4. Extracted data from OLTP, and NoSQL databases, transformed it and loaded it into the data warehouse, and then create an ETL pipeline.
    - extract data from OLTP, NoSQL, and MongoDB databases into CSV format. 
    - then transform the OLTP data to suit the data warehouse schema, and 
    - then load the transformed data into the data warehouse.

5. Create a Spark connection to the data warehouse, and then deploy a machine learning model.
    - use big data analytics to create a spark connection to the data warehouse, and 
    - then deploy a machine learning model on SparkML for making sales projections.

### Auxillary and Context
In order to get the IBM Data Engineering Professional Certificate I became competent in explaining and performing the key tasks required from a data engineer, database administrator, solutions architect, data platform developer, and data warehouse engineer. 

Gained **industrial experience with Python, SQL, Relational Databases, NoSQL Databases, Apache Spark, building a data pipeline, managing databases and working with data in data warehouses.** I was required to **design, deploy and manage structured and unstructured data** of an **end-to-end data engineering platform** consisting of various Relational (Analytical/Transactional Data Warehousing), NoSQL & Big Data repositories as well as data piplelines to connect them.

Industrial experience in technical skills such as;
- Utilizing **Python programming language** and **Linux/UNIX shell scripts** to extract, transform and load (**ETL**) data.
- Building ETL and ELT data pipelines with exposure to vendors like Apache Kafka, IBM, and AWS
- Working with Relational Databases (RDBMS) such as IBM DB2, PostgreSQL, MySQL with exposure to MS SQL and Oracle db 
- Querying data using Advanced SQL statements(i.e DDL, DML, DCL, DTL)
- Working with NoSQL databases such as MongoDB, IBM Cloudant, Cassandra, and CosmosDB with exposure to Redis, Neo4J and DynamoDB.
- Working with **Big Data**, **Machine Learning**, and **Big Data engines** like **Hadoop,Hive and Spark**.
- Developing and Administrating **Data Warehouses**
- Performing **Data Integration** with platform vendor IBM and exposure to vendors like Talend, SAP, Oracle, and Microsoft
- Utilized **Business Intelligence** tools to analyze and extract insights

<image src="https://github.com/kyzyto/IBM-DEC/blob/main/IBM-Data_Engineer_professional_certificate.pdf"/>


